{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Sam Dimmock\n",
    "- Vincent Gao\n",
    "- Owen Guan\n",
    "- Ryota Takemura\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Is it possible to design an AI algorithm that behaves similar to human players ranging from beginning to master for the simple-styled Shanghai mahjong? The data used represents training data by both simulations and real game data from online players, measured in terms of skill level, speed, defensive capabilities, and win potential. This data is used to guide the AI algorithm how to behave as an agent with a goal to win but with differentiating skill levels. The performance level will be measured by playing against other players, oracle agents, and computationally inefficient agents.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "\n",
    "\n",
    "Mahjong is an iconic multiplayer imperfect information game that has been played for nearly 2 centuries around the world, originating in China in the mid-1800s<a name=\"lang\"></a>[<sup>[1]</sup>](#langnote). One of the reasons it is so beloved by millions of players is because of the flexible rulesets, point system, complex strategies, and fun it brings to all players. However, it is a game involving very much stochasticity as each of the players try to find a winning set of 14 tiles. While there are certainly many mahjong AI algorithms like those used in online games such as Mahjong Soul and Microsoft’s Suphx, the goal is to create an optimized AI algorithm for Shanghai mahjong that is fair and behaves in a manner similar to humans.\n",
    "\n",
    "\n",
    "The best way to analyze training data is simply by using a trainer like those created by players such as Euophrys<a name=\"euophrys\"></a>[<sup>[2]</sup>](#euophrysnote) to scrape actual gameplay from online records (such as Mahjong Soul’s database) for different levels of play; this helps for understanding different methods of playing mahjong as well as different rulesets (Mahjong Soul’s style is called jantama/riichi mahjong). On the other hand, when designing an optimal algorithm, it is important to look into Suphx and the way deep reinforcement learning is used to formulate the best strategy.<a name=\"li\"></a>[<sup>[3]</sup>](#linote) The design approach was decided with techniques such as run-time policy adaptation, global reward prediction, and oracle guiding to reach the top 0.1% in terms of ranking on the Tenhou gaming service.<a name=\"li\"></a>[<sup>[3]</sup>](#linote) Due to the nature of the game having up to 53 tiles of the total 136 tiles being hidden at any given time.it is difficult to build a standard game tree so certain techniques such as Monte-Carlo tree search and search min-max algorithms have limited applications. The way Suphx circumvents these difficulties for jantama is through policy gradient algorithms, global reward prediction of a final reward of hypothetical future rounds via learning signals, agent based training on an oracle agent with perfect information that slowly decreases access until it becomes a normal agent, and finally, a parametric Monte-Carlo policy adaptation to improve run-time.<a name=\"li\"></a>[<sup>[3]</sup>](#linote) It is important to keep in mind that the difficulties experienced when designing an implementation for an optimal riichi mahjong AI may not be the exact same for a simplistic Shanghai mahjong ruleset, especially if the AI is more imitation/difficulty based in design.\n",
    "\n",
    "The core framework of mahjong can be compartmentalized to four key considerations that can be used for structuring an AI: mahjong is a multi-agent four player game with no teams; mahjong is imperfect information so nothing about the hidden tiles or opponents’ tiles are directly known; not including the 13 starting tiles that are known, the initiate possible states, there are nearly 5 x 10128 possible hidden states in the set of all states, forming a hidden Markov model for each player; and finally, the cost-reward application for mahjong ranges completely differently depending on the game state, as a good hand very much depends on the hidden markov model in order to determine possible winning hands.<a name=\"koyamada\"></a>[<sup>[4]</sup>](#koyamadanote) This framework can be applied to both riichi and Shanghai mahjong and will serve as the fundamentals for how training data is approached and how deep reinforcement learning will be used for this particular system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We implement Mahjong playing AI by using Monte Carlo on policy method. We do it with exploring start for AI to learn many tactics on a lot of kinds of situation because Mahjong is a somewhat highly stochastic game. Even though it is, the optimal movement is highly related to the winning. We set the reward not only on the winning and losing, which is outcome, but on Pung and Chi to measure the AI performance because those are considered as good ways to become closer to winning. Furthermore we will set some rules to teach AI what kind of movement leading to the good situation and bad situation to enhance it learning optimal movement efficiently as needed. To improve the value of win rate, AI will learn as the number of simulation increase. Like this AI uses Monte Carlo to get better performance on many kinds of situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "To implement our solution, we plan to use Monte Carlo methods to help train an AI to play the simplified version of Mahjong. First, we have to program this simplified version of the game. As stated in the problem statement, the objective of this game is to have a total of 4 straights or three of a kind and one pair. Each state will consist of the current hand of 13 tiles, what the opponents has tossed, and what tile has been drawn. The actions one can take is chi, pung, drawing a tile, discarding a tile, or nothing. The AI can only chi a tile if it's tossed by the player before it. The AI must draw a tile on it's turn. The AI can only discard a tile if they just drew a tile, chi, or pung. The AI can only do nothing if an opponent tossed out a tile that can't be chi or pung. The values for each action will be shown below: \n",
    "\n",
    "| **Action**        | **Reward** |\n",
    "|-------------------|------------|\n",
    "| **Pung or Chi**   | +0.1       |\n",
    "| **Draw, Discard, Nothing** | +0         |\n",
    "| **Win**           | +1         |\n",
    "| **Lose**          | -1         |\n",
    "\n",
    "The policy we will start with is to always pung or chi if possible and to discard tiles that are not a pair or within two of any tile of the same suit. \n",
    "\n",
    "We believe this solution will work because this game contains a lot of uncertainty and variability, making a complete model near impossible. This rules out any model-based reinforcement learning and dynamic programming. Instead, by playing the game hundreds of thousands of times, we can get closer and closer to an optimal policy to this simplified version of Mahjong. This is because as the number of simulations approaches infinity, the optimal policy gets closer and closer. \n",
    "\n",
    "The solution will be tested against a benchmark model that functions on randomness. Every action that is avaliable for them to do will be chosen randomly and the tile it discards will also be randomly chosen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The evaluation we will use is win rate. The objective of Mahjong is to win, so having a higher winner rate means the model is doing better than it's opponents. The way win rate will be calculated will be the total number of wins a model has divided by the total number of games played. \n",
    "\n",
    "\n",
    "$$Win Rate = \\Large\\frac{N_{win}}{N_{total}}$$\n",
    " \n",
    "Where:  \n",
    "- $N_{win}$ = Number of games won  \n",
    "- $N_{total}$= Total number of games played  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two potential ethical concerns with this project. One is that if it works well enough, it could be used to cheat in high-stakes games; the other is that someone could find a way of using part of our code to do something different. Neither of these are particularly concerning - people put a lot of effort into preventing people from cheating in high-stakes gambling, and the program we’re trying to make is similar enough to other existing programs that we’re unlikely to find anything particularly new that isn’t specific to this kind of game. If we were planning on deploying it (commercially or for free), we’d also have to worry about ensuring equal access for anyone who wants it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Coordinate through Instagram; check for messages regularly\n",
    "* Make all major decisions in group meetings (in-person or over Zoom), with the exception of when group meetings should be held\n",
    "* Group decisions must be acceptable to everyone - people will not be outvoted\n",
    "* Agree on ideal and fallback deadlines, taking people’s schedules into consideration\n",
    "* Everyone should do an equal amount of work overall - if you’re extra busy one week, make up for it before or afterwards\n",
    "* If anything comes up that affects the rest of the group, communicate through Instagram as soon as reasonably possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/11  |  7 PM |  Brainstorm topics/questions (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 2/18  |  7 PM |  A clear idea of what and how we will approach the programming of this project | Discuss and finalize the policy that the Monte Carlo method will follow | \n",
    "| 2/25  | 7 PM  |  Mahjong should be fully coded and ready for simulations | Begin coding the Monte Carlo methods   |\n",
    "| 3/4  | 7 PM  | Monte Carlo methods and begin simulations | Finalizing all the written stuff for the project   |\n",
    "| 3/11  | 7 PM  | Write conclusions and edit | Finalize the project and make any edits |\n",
    "| 3/17  | Before 11:59 PM  | N/A | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"langnote\"></a>1.[^](#lang) Lang, C. (2021, May 4). What the surprising history of Mah-jongg can teach us about America. TIME. https://time.com/6045817/mahjongg-history/\n",
    "\n",
    "<a name=\"euophrysnote\"></a>2.[^](#euophrys) Euophrys. (n.d.). GitHub - Euophrys/Riichi-Trainer: A Riichi Mahjong trainer. GitHub. https://github.com/Euophrys/Riichi-Trainer\n",
    "\n",
    "<a name=\"linote\"></a>3.[^](#li) Li, J., Koyamada, S., Ye, Q., Liu, G., Wang, C., Yang, R., Zhao, L., Qin, T., Liu, T., & Hon, H. (2021, January 25). Suphx: Mastering Mahjong with Deep Reinforcement Learning - Microsoft Research. Microsoft Research. https://www.microsoft.com/en-us/research/publication/suphx-mastering-mahjong-with-deep-reinforcement-learning/\n",
    "\n",
    "<a name=\"koyamadanote\"></a>2.[^](#koyamada) Koyamada, S., Habara, K., Goto, N., Okano, S., Nishimori, S., & Ishii, S. (n.d.). Mjx: A framework for Mahjong AI research. 2021 IEEE Conference on Games (CoG), 504–507. https://doi.org/10.1109/cog51982.2022.9893712"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
