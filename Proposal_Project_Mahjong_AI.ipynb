{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "You have the choice of doing either (1) an AI solve a problem style project or (2) run a Special Topics class on a topic of your choice.  If you want to do (2) you should fill out the _other_ proposal for that. This is the proposal description for (1).\n",
    "\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project. \n",
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like 8-Queens or a small Traveling Salesman Problem or similar\n",
    "- If its the kind of problem (e.g., RL) that interacts with a simulator or live task, then the problem will have a reasonably complex action space. For instance, a wupus world kind of thing with a 9x9 grid is definitely too small.  A simulated mountain car with a less complex 2-d road and simplified dynamics seems like a fairly low achievement level.  A more complex 3-d mountain car simulation with large extent and realistic dynamics, sure sounds great!\n",
    "- If its the kind of problem that uses a dataset, then the dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training an unsupervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "- The project must include some elements we talked about in the course\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your AI system. Generally RL tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible. \n",
    "- You will evaluate the performance of your AI system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n",
    "\n",
    "\n",
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Sam Dimmock\n",
    "- Vincent Gao\n",
    "- Owen Guan\n",
    "- Ryota Takemura\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Is it possible to design an AI algorithm that behaves similar to human players ranging from beginning to master for the simple-styled Shanghai mahjong? The data used represents training data by both simulations and real game data from online players, measured in terms of skill level, speed, defensive capabilities, and win potential. This data is used to guide the AI algorithm how to behave as an agent with a goal to win but with differentiating skill levels. The performance level will be measured by playing against other players, oracle agents, and computationally inefficient agents.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "\n",
    "\n",
    "Mahjong is an iconic multiplayer imperfect information game that has been played for nearly 2 centuries around the world, originating in China in the mid-1800s[1]. One of the reasons it is so beloved by millions of players is because of the flexible rulesets, point system, complex strategies, and fun it brings to all players. However, it is a game involving very much stochasticity as each of the players try to find a winning set of 14 tiles. While there are certainly many mahjong AI algorithms like those used in online games such as Mahjong Soul and Microsoft’s Suphx, the goal is to create an optimized AI algorithm for Shanghai mahjong that is fair and behaves in a manner similar to humans.\n",
    "\n",
    "\n",
    "The best way to analyze training data is simply by using a trainer like those created by players such as Euophrys[2] to scrape actual gameplay from online records (such as Mahjong Soul’s database) for different levels of play; this helps for understanding different methods of playing mahjong as well as different rulesets (Mahjong Soul’s style is called jantama/riichi mahjong). On the other hand, when designing an optimal algorithm, it is important to look into Suphx and the way deep reinforcement learning is used to formulate the best strategy.[3] The design approach was decided with techniques such as run-time policy adaptation, global reward prediction, and oracle guiding to reach the top 0.1% in terms of ranking on the Tenhou gaming service.[3] Due to the nature of the game having up to 53 tiles of the total 136 tiles being hidden at any given time.it is difficult to build a standard game tree so certain techniques such as Monte-Carlo tree search and search min-max algorithms have limited applications. The way Suphx circumvents these difficulties for jantama is through policy gradient algorithms, global reward prediction of a final reward of hypothetical future rounds via learning signals, agent based training on an oracle agent with perfect information that slowly decreases access until it becomes a normal agent, and finally, a parametric Monte-Carlo policy adaptation to improve run-time.[3] It is important to keep in mind that the difficulties experienced when designing an implementation for an optimal riichi mahjong AI may not be the exact same for a simplistic Shanghai mahjong ruleset, especially if the AI is more imitation/difficulty based in design.\n",
    "\n",
    "The core framework of mahjong can be compartmentalized to four key considerations that can be used for structuring an AI: mahjong is a multi-agent four player game with no teams; mahjong is imperfect information so nothing about the hidden tiles or opponents’ tiles are directly known; not including the 13 starting tiles that are known, the initiate possible states, there are nearly 5 x 10128 possible hidden states in the set of all states, forming a hidden Markov model for each player; and finally, the cost-reward application for mahjong ranges completely differently depending on the game state, as a good hand very much depends on the hidden markov model in order to determine possible winning hands.[4] This framework can be applied to both riichi and Shanghai mahjong and will serve as the fundamentals for how training data is approached and how deep reinforcement learning will be used for this particular system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once).\n",
    "\n",
    "We implement Mahjong playing AI by using Monte Carlo on policy method. We do it with exploring start for AI to learn many tactics on a lot of kinds of situation because Mahjong is a somewhat highly stochastic game. Even though it is, the optimal movement is highly related to the winning. We set the reward not only on the winning and losing, which is outcome, but on Pung and Chi to measure the AI performance because those are considered as good ways to become closer to winning. Furthermore we will set some rules to teach AI what kind of movement leading to the good situation and bad situation to enhance it learning optimal movement efficiently as needed. To improve the value of win rate, AI will learn as the number of simulation increase. Like this AI uses Monte Carlo to get better performance on many kinds of situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "You should have a strong idea of what dataset(s) will be used to accomplish this project. \n",
    "\n",
    "If you know what (some) of the data you will use, please give the following information for each dataset:\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc will be needed\n",
    "\n",
    "We don't yet know what dataset(s) will be so we describe the desire below.\n",
    "- link/reference to obtain it\n",
    "→online free accesible dataset\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "→Not so large to reduce the learning time, around several thousands.\n",
    "- what an observation consists of\n",
    "→players' current hands, current tiles on the table, and actions taken by players\n",
    "- what some critical variables are, how they are represented\n",
    "→Player hand (list of numbers), actions taken(numbers), the discarded tiles(numbers)\n",
    "- any special handling, transformations, cleaning, etc will be needed\n",
    "→we should unify to one mahjong rule and modify the dataset because the rule depends on the area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Why might your solution work? Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. \n",
    "\n",
    "To implement our solution, we plan to use Monte Carlo methods to help train an AI to play the simplified version of Mahjong. First, we have to program this simplified version of the game. As stated in the problem statement, the objective of this game is to have a total of 4 straights or three of a kind and one pair. Each state will consist of the current hand of 13 tiles, what the opponents has tossed, and what tile has been drawn. The actions one can take is chi, pung, drawing a tile, discarding a tile, or nothing. The AI can only chi a tile if it's tossed by the player before it. The AI must draw a tile on it's turn. The AI can only discard a tile if they just drew a tile, chi, or pung. The AI can only do nothing if an opponent tossed out a tile that can't be chi or pung. The values for each action will be shown below: \n",
    "\n",
    "| **Action**        | **Reward** |\n",
    "|-------------------|------------|\n",
    "| **Pung or Chi**   | +0.1       |\n",
    "| **Draw, Discard, Nothing** | +0         |\n",
    "| **Win**           | +1         |\n",
    "| **Lose**          | -1         |\n",
    "\n",
    "The policy we will start with is to always pung or chi if possible and to discard tiles that are not a pair or within two of any tile of the same suit. \n",
    "\n",
    "We believe this solution will work because this game contains a lot of uncertainty and variability, making a complete model near impossible. This rules out any model-based reinforcement learning and dynamic programming. Instead, by playing the game hundreds of thousands of times, we can get closer and closer to an optimal policy to this simplified version of Mahjong. This is because as the number of simulations approaches infinity, the optimal policy gets closer and closer. \n",
    "\n",
    "The solution will be tested against a benchmark model that functions on randomness. Every action that is avaliable for them to do will be chosen randomly and the tile it discards will also be randomly chosen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms).\n",
    "\n",
    "The evaluation we will use is win rate. The objective of Mahjong is to win, so having a higher winner rate means the model is doing better than it's opponents. The way win rate will be calculated will be the total number of wins a model has divided by the total number of games played. \n",
    "\n",
    "\n",
    "$$Win Rate = \\Large\\frac{N_{win}}{N_{total}}$$\n",
    " \n",
    "Where:  \n",
    "- $N_{win}$ = Number of games won  \n",
    "- $N_{total}$= Total number of games played  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination. Get creative!\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc...\n",
    "* *Team Expectation 1*\n",
    "* *Team Expectation 2*\n",
    "* *Team Expecation 3*\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this with something meaningful that is appropriate for your needs. It doesn't have to be something that fits this format.  It doesn't have to be set in stone... \"no battle plan survives contact with the enemy\". But you need a battle plan nonetheless, and you need to keep it updated so you understand what you are trying to accomplish, who's responsible for what, and what the expected due dates are for each item.\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/11  |  7 PM |  Brainstorm topics/questions (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 2/18  |  7 PM |  A clear idea of what and how we will approach the programming of this project | Discuss and finalize the policy that the Monte Carlo method will follow | \n",
    "| 2/25  | 7 PM  |  Mahjong should be fully coded and ready for simulations | Begin coding the Monte Carlo methods   |\n",
    "| 3/4  | 7 PM  | Monte Carlo methods and begin simulations | Finalizing all the written stuff for the project   |\n",
    "| 3/11  | 7 PM  | Write conclusions and edit | Finalize the project and make any edits |\n",
    "| 3/17  | Before 11:59 PM  | N/A | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "[1] Lang, C. (2021, May 4). What the surprising history of Mah-jongg can teach us about America. TIME. https://time.com/6045817/mahjongg-history/\n",
    "[2] Euophrys. (n.d.). GitHub - Euophrys/Riichi-Trainer: A Riichi Mahjong trainer. GitHub. https://github.com/Euophrys/Riichi-Trainer\n",
    "[3] Li, J., Koyamada, S., Ye, Q., Liu, G., Wang, C., Yang, R., Zhao, L., Qin, T., Liu, T., & Hon, H. (2021, January 25). Suphx: Mastering Mahjong with Deep Reinforcement Learning - Microsoft Research. Microsoft Research. https://www.microsoft.com/en-us/research/publication/suphx-mastering-mahjong-with-deep-reinforcement-learning/\n",
    "[4] Koyamada, S., Habara, K., Goto, N., Okano, S., Nishimori, S., & Ishii, S. (n.d.). Mjx: A framework for Mahjong AI research. 2021 IEEE Conference on Games (CoG), 504–507. https://doi.org/10.1109/cog51982.2022.9893712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
